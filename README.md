# ğŸš€ AWS End-to-End Data Lakehouse Analytics Platform

<p align="center">
  <a href="https://github.com/MinhKhoi3104/aws-end-to-end-data-lakehouse-analytics-platform/tree/main/README/#-quick-start-guide">
  <img src="https://img.shields.io/badge/project-ğŸš€quick_start-blue?style=for-the-badge&logo=github" alt="Quick Start Guide"/>
</a>
  <a href="https://github.com/MinhKhoi3104/aws-end-to-end-data-lakehouse-analytics-platform/tree/main/_002_src">
      <img src="https://img.shields.io/badge/project-source_code-green?style=for-the-badge&logo=github" alt="Sublime's custom image"/>
  </a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Apache%20Spark-3.5.3-orange?style=plastic&logo=apachespark&logoColor=white"/>
  <img src="https://img.shields.io/badge/Apache%20Airflow-2.10.3-017CEE?style=plastic&logo=apacheairflow&logoColor=white"/>
  <img src="https://img.shields.io/badge/Apache%20Iceberg-1.5.2-4B9CD3?style=plastic&logo=apache&logoColor=white"/>
  <img src="https://img.shields.io/badge/Amazon%20Redshift-Serverless-8C4FFF?style=plastic&logo=amazonredshift&logoColor=white"/>
  <img src="https://img.shields.io/badge/Amazon%20S3-Data%20Lake-569A31?style=plastic&logo=amazons3&logoColor=white"/>
  <img src="https://img.shields.io/badge/AWS%20Glue-Data%20Catalog-FF9900?style=plastic&logo=amazonaws&logoColor=white"/>
  <img src="https://img.shields.io/badge/PostgreSQL-15-4169E1?style=plastic&logo=postgresql&logoColor=white"/>
  <img src="https://img.shields.io/badge/dbt-Latest-FF694B?style=plastic&logo=dbt&logoColor=white"/>
  <img src="https://img.shields.io/badge/Apache%20Superset-Latest-20A7C9?style=plastic&logo=apache&logoColor=white"/>
  <img src="https://img.shields.io/badge/Grafana-Latest-F46800?style=plastic&logo=grafana&logoColor=white"/>
  <img src="https://img.shields.io/badge/Prometheus-2.38.0-E6522C?style=plastic&logo=prometheus&logoColor=white"/>
  <img src="https://img.shields.io/badge/Terraform-%3E%3D1.0-844FBA?style=plastic&logo=terraform&logoColor=white"/>
</p>


This project implements an **AWS-based Data Lakehouse platform** for processing and analyzing large-scale user search and behavior data across **Internet TV, OTT, and online entertainment platforms**. The system follows a **Lakehouse architecture on Amazon S3** using **Apache Iceberg** as the table format, while data ingestion and transformation across the **Bronze, Silver, and Gold layers** are performed using **Apache Spark (PySpark)**. At the **Gold layer**, data is modeled into **fact and dimension tables using a star schema**, providing curated and analytics-ready datasets.

The Gold-layer datasets are then **replicated into PostgreSQL**, which serves as the **analytical serving layer** for downstream consumption. Within PostgreSQL, **dbt** is used to apply business transformations, build **datamarts**, and define analytical metrics in a structured and version-controlled manner. These datamarts are subsequently consumed by **BI and visualization tools (Apache Superset)**, enabling efficient OLAP-style analysis and reporting. The data pipeline is orchestrated using **Apache Airflow**, with infrastructure provisioned via **Terraform (Infrastructure as Code)** on AWS to ensure scalable, reproducible environments, and system observability supported by **Grafana and Prometheus**.

![e2e_project_overview](/image/e2e_project_overview.png)
<p align="center">
  <em>End-to-End Project Overview</em>
</p>


## ğŸ“‹ Table of Contents
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ“š Dataset](#-dataset)
  - [Customer search data log](#customer-search-data-log)
  - [Crawled data from film website](#crawled-data-from-film-website)
- [ğŸŒ Architecture Overview](#-architecture-overview)
  - [1. AWS Configuration (Infrastructure as Code - Terraform)](#-aws-configuration-)
  - [2. Distributed Batch Processing](#2-distributed-batch-processing)
  - [3. Monitoring & Observability](#3-monitoring--observability)
  - [4. Datamart for business analytics and reporting](#4-datamart-for-business-analytics-and-reporting)
  - [5. Business Intelligence & Visualization](#5-business-intelligence--visualization)
- [ğŸš€ Quick Start Guide](#-quick-start-guide)
  - [Step 1: Infrastructure Setup with Terraform](#step-1-infrastructure-sepup-with-terraform)
  - [Step 2: Create Docker Network](#step-2-create-docker-network)
  - [Step 3: Start PostgreSQL and Monitoring Services](#step-3-start-postgresql-and-monitoring-services)
  - [Step 4: Configure Airflow Environment](#step-4-configure-airflow-environment)
  - [Step 5: Build and Start Airflow](#step-5-build-and-start-airflow)
  - [Step 6: Import Grafana Dashboards](#step-4-configure-airflow-environment)
  - [Step 7: Run Data Pipeline](#step-7-run-data-pipeline)
  - [Step 8: Replicate Data to PostgreSQL](#step-8-replicate-data-to-postgresql)
  - [Step 9: Build Datamart with dbt](#step-9-build-datamart-with-dbt)
  - [Step 10: Visualize data by Apache Superset and Reporting](#step-10-visualize-data-by-apache-superset-and-reporting)
- [ğŸ”§ Key Technologies](#-key-technologies)
- [ğŸ“ƒ License](#-license)

## ğŸ“ Project Structure

```
aws-end-to-end-data-lakehouse-analytics-platform/
â”œâ”€â”€ _000_data/                    # Sample data files
â”‚   â”œâ”€â”€ crawl_data/               # Crawled reference data
â”‚   â””â”€â”€ customer_search_log_data/ # Raw customer search logs
â”‚
â”œâ”€â”€ _001_iac/                     # Infrastructure as Code
â”‚   â””â”€â”€ terraform/
â”‚       â”œâ”€â”€ bootstrap/            # Terraform state bucket
â”‚       â”œâ”€â”€ s3/                   # S3 data lake buckets
â”‚       â””â”€â”€ redshift/            # Redshift Serverless
â”‚
â”œâ”€â”€ _002_src/                     # Source code
â”‚   â”œâ”€â”€ build_datamart/          # dbt project
â”‚   â”‚   â””â”€â”€ dbt_customer_behaviour_analytics_dmt/
â”‚   â”œâ”€â”€ crawl_web_data/          # Web scraping scripts
â”‚   â”œâ”€â”€ monitoring/              # Grafana & Prometheus configs
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ grafana/
â”‚   â””â”€â”€ orchestration/           # Airflow DAGs and ETL jobs
â”‚       â”œâ”€â”€ dags/
â”‚       â”œâ”€â”€ data_pipeline/
â”‚       â””â”€â”€ jars/
â”‚
â”œâ”€â”€ _003_test/                    # Test utilities
â”‚   â””â”€â”€ data_pipeline/
â”‚
â”œâ”€â”€ _004_docs/                    # Documentation
â”‚
â”œâ”€â”€ docker-compose.dmt.yml        # PostgreSQL & pgAdmin
â”œâ”€â”€ docker-compose.grafana.yml   # Grafana & Prometheus
â”œâ”€â”€ docker-compose.orchestration.yml # Airflow (in orchestration/)
â””â”€â”€ README.md                     # This file
```

---

## ğŸ“š Dataset
### Customer search data log
Dá»¯ liá»‡u Ä‘Æ°á»£c dÃ¹ng cho dá»± Ã¡n nÃ y lÃ  dá»¯ liá»‡u customer searching Ä‘Æ°á»£c log tá»« 1 online entertainment platforms cÃ³ thá»ƒ dÃ¹ng trÃªn nhiá»u thiáº¿t bá»‹ (mÃ¡y tÃ­nh, Ä‘iá»‡n thoáº¡i, Tivi,...) tá»« ngÃ y 2022-06-01 Ä‘áº¿n ngÃ y 2022-06-03. Dá»¯ liá»‡u cho biáº¿t Ä‘Æ°á»£c lá»‹ch sá»­ searching cá»§a customer trong thá»i gian sá»­ dá»¥ng dá»‹ch vá»¥ trÃªn ná»n táº£ng vá»›i cáº¥u trÃºc vÃ  Ã½ nghÄ©a nhÆ° sau:

| Field          | Description                                                          |
| -------------- | -------------------------------------------------------------------- |
| event_time     | MÃ£ ID duy nháº¥t cho má»—i sá»± kiá»‡n log                                   |
| datetime       | Thá»i Ä‘iá»ƒm sá»± kiá»‡n xáº£y ra (timestamp)                                 |
| user_id        | ID ngÆ°á»i dÃ¹ng (cÃ³ thá»ƒ None náº¿u chÆ°a Ä‘Äƒng nháº­p / guest)               |
| keyword        | Tá»« khoÃ¡ user sá»­ dá»¥ng Ä‘á»ƒ search                                       |
| category       | Loáº¡i hÃ nh vi (enter / quit ) â€” cÃ³ thá»ƒ lÃ  tráº¡ng thÃ¡i session          |
| proxy_isp      | NhÃ  máº¡ng / ISP mÃ  user sá»­ dá»¥ng (fpt / vnpt / viettel/ other / spt)   |
| platform       | Thiáº¿t bá»‹ / há»‡ Ä‘iá»u hÃ nh (android / ios / smarttv-sony-androidâ€¦)      |
| networkType    | Loáº¡i káº¿t ná»‘i ('wifi', 'WWAN', 'ethernet','3g', ...)                  |
| action         | HÃ nh Ä‘á»™ng chÃ­nh (search)                                             |
| userPlansMap   | Danh sÃ¡ch gÃ³i dá»‹ch vá»¥ hiá»‡n táº¡i cá»§a user                              |

![customer_search_log_data](/image/customer_search_log_data.png)
<p align="center">
  <em>Customer Search Log Data Sample</em>
</p>

### Crawled data from film website
Dá»¯ liá»‡u Ä‘Æ°á»£c crawl tá»« 1 web film online, dá»¯ liá»‡u Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ chuáº©n hÃ³a dá»¯ liá»‡u search cá»§a customer báº±ng thuáº­t toÃ¡n Machine Learning Ä‘á»ƒ cÃ³ thá»ƒ chuáº©n hÃ³a dá»¯ liá»‡u search vÃ­ dá»¥ nhÆ°: cÃ³ 2 user Ä‘á»u tÃ¬m phim Doraemon, trong Ä‘Ã³ user_1 search kÃ½ tá»± 'doramon' (máº¥t chá»¯ 'e') vÃ  user_2 search kÃ½ tá»± 'doremon' (máº¥t chá»¯ 'a') thÃ¬ dá»±a vÃ o viá»‡c Ã¡p dá»¥ng ML, tá»« khÃ³a cá»§a 2 user nÃ y sáº½ Ä‘Æ°á»£c chuáº©n hÃ³a vá» loáº¡i phim Ä‘Ãºng cáº§n tÃ¬m lÃ  'Doraemon'.

Cáº¥u trÃºc cá»§a dá»¯ liá»‡u crawl vá» lÃ :

| Field          | Description                                                          |
| -------------- | -------------------------------------------------------------------- |
| _id            | ID Ä‘á»‹nh danh duy nháº¥t cá»§a bá»™ phim                                    |
| title          | TÃªn phim hiá»ƒn thá»‹ cho ngÆ°á»i dÃ¹ng (Tiáº¿ng viá»‡t)                        |
| slug           | Chuá»—i Ä‘á»‹nh danh thÃ¢n thiá»‡n vá»›i URL (SEO-friendly)                    |
| original_title | TÃªn gá»‘c cá»§a phim theo ngÃ´n ngá»¯ sáº£n xuáº¥t                              |
| release_date   | NgÃ y phim chÃ­nh thá»©c phÃ¡t hÃ nh hoáº·c báº¯t Ä‘áº§u chiáº¿u                    |
| status         | Tráº¡ng thÃ¡i phÃ¡t hÃ nh cá»§a phim                                        |
| quality        | Cháº¥t lÆ°á»£ng video cao nháº¥t hiá»‡n cÃ³                                    |
| rating         | PhÃ¢n loáº¡i Ä‘á»™ tuá»•i ngÆ°á»i xem (Age Rating)                             |
| runtime        | Thá»i lÆ°á»£ng má»—i táº­p hoáº·c toÃ n bá»™ phim                                 |
| overview       | MÃ´ táº£ ngáº¯n / tÃ³m táº¯t ná»™i dung phim                                   |
| origin_country | Quá»‘c gia sáº£n xuáº¥t phim                                               |
| genres         | Thá»ƒ loáº¡i phim                                                        |

![crawled_data](/image/crawled_data.png)
<p align="center">
  <em>Crawled Data Sample</em>
</p>

---

## ğŸŒ Architecture Overview
### 1. AWS Configuration (Infrastructure as Code - Terraform)

Dá»± Ã¡n nÃ y sá»­ dá»¥ng Terraform nhÆ° má»™t cÃ´ng cá»¥ Infrastructure as Code (IaC) Ä‘á»ƒ Ä‘á»‹nh nghÄ©a, cáº¥u hÃ¬nh vÃ  quáº£n lÃ½ háº¡ táº§ng cÅ©ng nhÆ° cÃ¡c tÃ i nguyÃªn Ä‘Ã¡m mÃ¢y trÃªn AWS (bao gá»“m Amazon S3 vÃ  Amazon Redshift), nháº±m táº¡o ra cÃ¡c mÃ´i trÆ°á»ng nháº¥t quÃ¡n, Ä‘Æ°á»£c kiá»ƒm soÃ¡t phiÃªn báº£n vÃ  cÃ³ thá»ƒ tÃ¡i táº¡o má»™t cÃ¡ch tá»± Ä‘á»™ng.

1. [ğŸ”¨ Infrastructure Code â€“ Configure AWS Architecture using Terraform](/_001_iac/terraform/)

TÃ i liá»‡u mÃ´ táº£ chi tiáº¿t cÃ¡ch tá»• chá»©c mÃ£ nguá»“n, cáº¥u hÃ¬nh tÃ i nguyÃªn AWS Ä‘Æ°á»£c Ã¡p dá»¥ng trong Terraform cho dá»± Ã¡n nÃ y:

2. [ğŸ“ƒ Documents - Terraform Documentation](/_004_docs/README-terraform.md)

Directory Structure:

```
_001_iac/terraform/
â”œâ”€â”€ bootstrap/          # First module to deploy - creates the S3 bucket used to store Terraform state (backend)
â”œâ”€â”€ s3/                # Module creates S3 data lake buckets
â””â”€â”€ redshift/          # Module creates Redshift Serverless infrastructure
```

### 2. Distributed Batch Processing
Data Flow:

```
Raw Data (S3)
    â†“
Bronze Layer (S3 Parquet)
    â†“
Silver Layer (S3 Parquet)
    â†“
Gold Layer (Redshift + Iceberg)
    â†“
PostgreSQL (Replication)
    â†“
dbt Datamart (PostgreSQL)
    â†“
BI Tools (Superset)
```

Dá»± Ã¡n nÃ y triá»ƒn khai kiáº¿n â€‹â€‹trÃºc xá»­ lÃ½ hÃ ng loáº¡t phÃ¢n tÃ¡n máº¡nh máº½ sá»­ dá»¥ng PySpark Ä‘á»ƒ tÃ­nh toÃ¡n vÃ  Apache Airflow Ä‘á»ƒ Ä‘iá»u phá»‘i. BÃªn cáº¡nh Ä‘Ã³, Apache Iceberg is used as the table format on Amazon S3, providing enterprise-grade capabilities such as ACID transactions, time travel, schema evolution, and partition evolution, ensuring reliable and maintainable datasets. CÃ¡c chá»©c nÄƒng cá»‘t lÃµi Ä‘Æ°á»£c cáº¥u trÃºc nhÆ° sau:


1. [ğŸ”¨ Code â€“ Data Pipeline (OLTP -> Data Lakehouse & Data Warehouse)](/_002_src/orchestration/data_pipeline/)

2. [ğŸ“ƒ Documents - Data Lakehouse & Warehouse Architecture Documentation](/_004_docs/README-data-lakehouse-&-warehouse-architecture.md)

Dá»¯ liá»‡u hÃ ng ngÃ y sáº½ Ä‘Æ°á»£c sá»­ lÃ½ dá»±a trÃªn Pyspark, dá»¯ liá»‡u Ä‘Æ°á»£c xá»­ lÃ½ qua 3 táº§ng theo mÃ´ hÃ¬nh Medallion Architecture , where data is processed sequentially across three layers: **Bronze, Silver, and Gold**. This architectural approach ensures data quality, consistency, scalability, and end-to-end data lineage throughout the entire pipeline. At the Gold layer, data is modeled using a star schema with fact and dimension tables, enabling the construction of subject-oriented datamarts** that efficiently support OLAP analysis and BI reporting. Apache Iceberg Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m Ä‘á»‹nh dáº¡ng báº£ng trÃªn Amazon S3 á»Ÿ Gold Layer, cung cáº¥p cÃ¡c kháº£ nÄƒng cáº¥p doanh nghiá»‡p nhÆ° ACID transactions, time travel, schema evolution, and partition evolution, ensuring reliable and maintainable datasets.

<p align="center">
  <img src="/image/Medallion_Architect.png" alt="Medallion Architect" />
</p>

3. [ğŸ”¨ Code â€“ Scheduling based on Airflow (DAGs)](/_002_src/orchestration/dags/)
4. [ğŸ“ƒ Documents - Airflow Documentation](/_004_docs/README-airflow.md)

ToÃ n bá»™ quy trÃ¬nh xá»­ lÃ½ hÃ ng loáº¡t Ä‘Æ°á»£c tá»± Ä‘á»™ng hÃ³a thÃ´ng qua Apache Airflow, vá»›i cÃ¡c DAG Ä‘Æ°á»£c lÃªn lá»‹ch cháº¡y hÃ ng Ä‘Ãªm lÃºc 2:00 sÃ¡ng. Bá»™ láº­p lá»‹ch Ä‘iá»u phá»‘i 2 Dags bao gá»“m DAG 'data_pipeline_daily' dÃ¹ng Ä‘á»ƒ cháº¡y cÃ¡c job data pipeline hÃ ng ngÃ y (bao gá»“m Ä‘á»c dá»¯ liá»‡u tá»« nguá»“n, xá»­ lÃ½ dá»¯ liá»‡u qua cÃ¡c táº§ng vÃ  mÃ´ hÃ¬nh hÃ³a dá»¯ liá»‡u) vÃ  DAG 'redshift_to_postgre' dÃ¹ng Ä‘á»ƒ replicate dá»¯ liá»‡u tá»« táº§ng Gold vá» PostgreSQL Ä‘á»ƒ dÃ¹ng cho viá»‡c xÃ¢y dá»±ng Datamart vÃ  visualization.

Directory Structure:

```
orchestration/
â”œâ”€â”€ dags/                          # Airflow DAG definitions
â”‚   â”œâ”€â”€ data_pipeline.py           # Main ETL pipeline DAG
â”‚   â””â”€â”€ redshift_to_postgre.py    # Redshift â†’ PostgreSQL replication DAG
â”œâ”€â”€ data_pipeline/                 # ETL jobs source code
â”‚   â”œâ”€â”€ _01_config/                # Configuration files
â”‚   â”‚   â”œâ”€â”€ data_storage_config.py # S3, Redshift, PostgreSQL configs
â”‚   â”‚   â””â”€â”€ jar_paths.py          # JAR file paths
â”‚   â”œâ”€â”€ _02_utils/                 # Utility functions
â”‚   â”‚   â”œâ”€â”€ utils.py              # Spark session builders, S3/Redshift utils
â”‚   â”‚   â””â”€â”€ surrogate_key_registry.py # Surrogate key management
â”‚   â””â”€â”€ _03_etl_jobs/             # ETL job implementations
â”‚       â”œâ”€â”€ _0301_bronze/         # Bronze layer jobs
â”‚       â”œâ”€â”€ _0302_silver/         # Silver layer jobs
â”‚       â””â”€â”€ _0303_gold/           # Gold layer jobs (dimensions & facts)
â”œâ”€â”€ jars/                          # Required JAR dependencies
â”œâ”€â”€ Dockerfile                     # Airflow container image
â”œâ”€â”€ docker-compose.orchestration.yml # Docker Compose configuration
â””â”€â”€ requirements.txt               # Python dependencies
```

### 3. Monitoring & Observability
### 4. Datamart for business analytics and reporting
### 5. Business Intelligence & Visualization
---
## ğŸš€ Quick Start Guide

### Step 1: Infrastructure Setup with Terraform

Deploy AWS infrastructure in the following order:

#### 1.1 AWS Configuration (Required)

Terraform requires valid AWS credentials to provision resources.
Configure AWS access using one of the following methods (recommended: AWS CLI profile).

```bash
aws configure
```

Then, Ä‘iá»n cÃ¡c thÃ´ng tin nÃ y dá»±a trÃªn thÃ´ng tin AWS credentials cá»§a báº¡n:

```
AWS Access Key ID: {AWS_ACCESS_KEY_ID} 
AWS Secret Access Key: {AWS_SECRET_ACCESS_KEY} 
Default region name: ${AWS_DEFAULT_REGION} 
Default output format: json
```

#### 1.2 Bootstrap (Required First)

Creates S3 bucket for Terraform state:

```bash
cd _001_iac/terraform/bootstrap
terraform init
terraform plan
terraform apply
```

**Resources Created:**
- S3 bucket: `data-pipeline-e2e-terraform-state` (for storing Terraform state)

#### 1.3 S3 Data Lake

Creates S3 buckets for data storage:

```bash
cd _001_iac/terraform/s3
terraform init
terraform plan
terraform apply
```

**Resources Created:**
- Main data bucket: `data-pipeline-e2e-datalake-{random-suffix}`
- Log bucket: `s3-access-logs-{random-suffix}`

#### 1.4 Redshift Serverless

Creates Redshift Serverless infrastructure:

```bash
cd _001_iac/terraform/redshift
terraform init
terraform plan
terraform apply
```

**Resources Created:**
- VPC with 3 subnets across AZs
- Security groups
- IAM roles and policies
- Redshift Serverless namespace and workgroup

**ğŸ“– Detailed Documentation:** See [Terraform Infrastructure Documentation](_004_docs/README-terraform.md)

![Infrastructure Setup with Terraform](/image/s3_buckets.png)
<p align="center">
  <em>Infrastructure Setup with Terraform Sample Output</em>
</p>

---

### Step 2: Create Docker Network

Create the shared Docker network for all services:

```bash
docker network create aws_e2e_network
```

---

### Step 3: Start PostgreSQL and Monitoring Services

#### 3.1 Start PostgreSQL (for Airflow metadata and datamart)

```bash
docker-compose -f docker-compose.dmt.yml up -d --build
```

**Services Started:**
- PostgreSQL (port 5432)
- pgAdmin (port 5050)
- Postgres Exporter (port 9187)

#### 3.2 Start Monitoring Stack (Grafana + Prometheus)

```bash
docker-compose -f docker-compose.grafana.yml up -d --build
```

**Services Started:**
- Prometheus (port 9090)
- Grafana (port 3000)
- StatsD Exporter (ports 9125/udp, 9102)

**Access:**
- Grafana: http://localhost:3000 (admin/grafana)
- Prometheus: http://localhost:9090

**ğŸ“– Detailed Documentation:** See [Grafana Monitoring Documentation](_004_docs/README-grafana.md)

---

### Step 4: Configure Airflow Environment

#### 4.1 Create Environment File

Create `.env.aws` in `_002_src/orchestration/`:

```bash
cd _002_src/orchestration
cat > .env.aws << EOF
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_DEFAULT_REGION=ap-southeast-1

# Redshift (if override needed)
REDSHIFT_HOST=your-redshift-host.redshift-serverless.amazonaws.com
REDSHIFT_DB=my-project-e2e-dtb
REDSHIFT_USER=admin
REDSHIFT_PASSWORD=your-password
EOF
```

#### 4.2 Update Data Storage Configuration

Edit `_002_src/orchestration/data_pipeline/_01_config/data_storage_config.py`:

- Update `S3_DATALAKE_PATH` with your S3 bucket name
- Update `REDSHIFT_HOST`, `REDSHIFT_DB`, credentials
- Update `REDSHIFT_IAM_ROLE_ARN` with your IAM role ARN

---

### Step 5: Build and Start Airflow

#### 5.1 Build Docker Image

```bash
cd _002_src/orchestration
docker-compose -f docker-compose.orchestration.yml build
```

#### 5.2 Initialize Airflow Database

```bash
docker-compose -f docker-compose.orchestration.yml up airflow-init
```

#### 5.3 Start Airflow Services

```bash
docker-compose -f docker-compose.orchestration.yml up -d
```

**Services Started:**
- Airflow Webserver (port 8080)
- Airflow Scheduler
- Spark Master (port 7077, 8081)
- Spark Worker (port 8082)

**Access:**
- Airflow UI: http://localhost:8080 (admin/admin)

![Airflow UI](/image/airflow_ui.png)

**ğŸ“– Detailed Documentation:** See [Apache Airflow Orchestration Documentation](_004_docs/README-airflow.md)

---

### Step 6: Import Grafana Dashboards

1. Access Grafana: http://localhost:3000
2. Login with admin/grafana
3. Configure Prometheus datasource:
   - Go to **Configuration** â†’ **Data Sources**
   - Add Prometheus datasource: `http://prometheus:9090`
4. Import dashboards:
   - **Airflow Dashboard**: Import `_002_src/monitoring/grafana/airflow-cluster-dashboard.json`
   - **PostgreSQL Dashboard**: Import `_002_src/monitoring/grafana/postgresql-dashboard.json`

![grafana_airflow_dashboard](/image/grafana_airflow_dashboard.png)
<p align="center">
  <em>Grafana Airflow Monitoring Dashboard</em>
</p>

![grafana_postgres_dashboard](/image/grafana_postgres_dashboard.png)
<p align="center">
  <em>Grafana Postgres Monitoring Dashboard</em>
</p>

---

### Step 7: Run Data Pipeline

#### 7.1 Trigger Airflow DAG

1. Access Airflow UI: http://localhost:8080
2. Enable DAG: `data_pipeline_daily`
3. Trigger DAG with config:
   ```json
   {
     "etl_date": "20220601"
   }
   ```

The pipeline will:
- **Bronze Layer**: Ingest raw data from S3
- **Silver Layer**: Clean and normalize data
- **Gold Layer**: Create dimension and fact tables in Redshift and Iceberg

![airflow_pipeline_daily](/image/airflow_pipeline_daily.png)

#### 7.2 Monitor Pipeline Execution

- View DAG progress in Airflow UI
- Check Grafana dashboards for system metrics
- Review logs in Airflow task logs

**ğŸ“– Detailed Documentation:** See [Data Lakehouse and Warehouse Architecture Documentation](/_004_docs/README-data-lakehouse-&-warehouse-architecture.md)

---

### Step 8: Replicate Data to PostgreSQL

After Gold layer completes, trigger the replication DAG:

1. Enable DAG: `redshift_to_postgre`
2. Trigger manually (it will wait for `data_pipeline_daily.gold_finish`)

This DAG replicates all Gold layer tables from Redshift to PostgreSQL at schema 'dwh_user_search'.

![airflow_redshift_to_postgre](/image/airflow_redshift_to_postgre.png)

---

### Step 9: Build Datamart with dbt

#### 9.1 Install dbt libs

```bash
cd _002_src/build_datamart
pip install -r requirements.txt
```

#### 9.2 Configure dbt Profile

```bash
# Manually create the .dbt directory and set up profile.yml file
mkdir -p ~/.dbt;
# open folder dbt
cd ~/.dbt/ && code .
```

Create profile.yml and add this content into profile.yml
``` python
dbt_customer_behaviour_analytics_dmt:
  outputs:
    dev:
      type: postgres
      host: localhost
      port: 5432
      user: admin
      password: "admin"
      dbname: postgres
      schema: datamart

    prod:
      type: postgres
      host: localhost
      port: 5432
      user: admin
      password: "admin"
      dbname: postgres
      schema: datamart

  target: dev
```

#### 9.3 Run dbt Models

```bash
cd _002_src/build_datamart/dbt_customer_behaviour_analytics_dmt

# Install dependencies
dbt deps

# Run all models
dbt run

# Run tests
dbt test

# Generate documentation
dbt docs generate
dbt docs serve
```

**Models Created:**
- `dmt_search_event_base`
- `dmt_search_event_plan`
- `dmt_search_event_category`

![datamart](/image/datamart.png)


**ğŸ“– Detailed Documentation:** See [Build Datamart by using DBT](_004_docs/README-dbt.md)

---

### Step 10: Visualize data by Apache Superset and Reporting
Thá»±c hiá»‡n láº¥y dá»¯ liá»‡u tá»« Data Warehouse vÃ  Datamart Ä‘á»ƒ visualize thá»±c hiá»‡n bÃ¡o cÃ¡o Ä‘á»ƒ tá»« Ä‘Ã³ get insight tá»« hÃ nh vi cá»§a user Ä‘á»ƒ tá»« Ä‘Ã³ cÃ³ thá»ƒ Ä‘Æ°a ra Ä‘Æ°á»£c cÃ¡c quyáº¿t Ä‘á»‹nh, chÃ­nh sÃ¡ch há»£p lÃ½ Ä‘á»ƒ nÃ¢ng cao sá»‘ lÆ°á»£ng ngÆ°á»i Ä‘Äƒng kÃ½, nÃ¢ng cao cháº¥t lÆ°á»£ng ngÆ°á»i dÃ¹ng vÃ  phÃ¡t triá»ƒn lá»£i nhuáº­n cá»§a doanh nghiá»‡p.

You can access the dashboard from the link: [Customer_Behaviour_Analyst_Dashboard](https://misfashioned-premonarchial-nguyet.ngrok-free.dev/superset/dashboard/12)

![superset_dashboard](/image/superset_dashboard.jpg)
<p align="center">
  <em>Customer Behaviour Analyst Dashboard</em>
</p>

**ğŸ“– Detailed Documentation:** See [Visualize by using Apache Superset and Report](_004_docs/)

---

## ğŸ”§ Key Technologies

| Technology | Version | Purpose |
|------------|---------|---------|
| Apache Spark | 3.5.3 | Distributed data processing |
| Apache Airflow | 2.10.3 | Workflow orchestration |
| Apache Iceberg | 1.5.2 | Open table format for ACID transactions |
| AWS Redshift Serverless | Latest | Serverless data warehouse |
| AWS S3 | - | Object storage for data lake |
| PostgreSQL | 15 | Metadata DB and BI datamart |
| dbt | Latest | Data transformation and modeling |
| Grafana | Latest | Metrics visualization |
| Prometheus | 2.38.0 | Metrics collection |
| Terraform | >= 1.0 | Infrastructure as Code |
| Terraform | >= 1.0 | Infrastructure as Code |
| Apache Superset | Latest | Visualization and Reports |

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](/LICENSE) file for details.

---
