# dbt Application

## GENERAL 
This project used dbt to transform warehouse data into datamart. dbt handles the heavy lifting by creating modular, maintainable data models that power analytics.

## WHY USING DBT
1. Easily configure a materialization for each model
Handles boilerplate SQL
Turns SELECT queries into tables, views, or incremental models
2. Use a code compiler
Allows SQL files to include Jinja.
Enables repeated SQL to be shared through macros.
3. Determine the order of model execution
Provides the ref() function to reference other models instead of selecting from existing tables in the warehouse.
Determines the correct execution order.
4. Built-in documentation
Provides a mechanism to write, version-control, and share documentation for dbt models.
Supports plain text and markdown formatting.
5. Models testing
Improves the integrity of the SQL models.
Provides built-in tests to validate the results generated by a model.
6. Package management
includes a built-in package manager.
Allows teams to use and publish both public and private repositories of dbt code.
7. Load seed files
Seed files are CSV files stored within the dbt project.
Commonly used for:
Mapping values
Static or infrequently changing enrichment data
8. Snapshot mutable data
Provides a mechanism to snapshot raw data for a point in time, through use of snapshots.
Implements type-2 Slowly Changing Dimensions over mutable source tables.

## ARCHITECTURE AND CONFIGURATION
### Architecture
build_datamart/
│
├── profile.yml
├── requirements.txt
└── dbt_customer_behaviour_analytics_dmt/
         │
         ├── dbt_project.yml
         ├── models/
         │   ├── source/
         │   │   └── gold_sources.yml
         │   ├── dmt_search_event_base.sql
         │   ├── dmt_search_event_category.sql
         │   ├── dmt_search_event_plan.sql
         │   └── schema.yml
         │
         ├── tests/
         ├── macros/
         ├── seeds/
         ├── snapshots/
         └── analyses/

### Configuration
**Profile Configuration:**
Stores warehouse connection settings:
''' python
dbt_customer_behaviour_analytics_dmt:
  outputs:
    dev:
      type: postgres
      host: localhost
      port: 5432
      user: admin
      password: "admin"
      dbname: postgres
      schema: datamart

    prod:
      type: postgres
      host: localhost
      port: 5432
      user: admin
      password: "admin"
      dbname: postgres
      schema: datamart

  target: dev
'''

**Project configuration:**
dbt is configured in the dbt_project.yml: 

''' python
name: 'dbt_customer_behaviour_analytics_dmt'
version: '1.0.0'
config-version: 2

profile: 'dbt_customer_behaviour_analytics_dmt'

model-paths: ["models"]
analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]

clean-targets:         # directories to be removed by `dbt clean`
  - "target"
  - "dbt_packages"

models:
  dbt_customer_behaviour_analytics_dmt:
    +materialized: table
'''

### Core Components
**Source of models:**
Datamart is built from the gold layer of the data warehouse.
**Models:**
Datamart includes three tables supporting monthly analytics. These tables are built by SQL files, including:
- dmt_search_event_base.sql
- dmt_search_event_plan.sql
- dmt_search_event_category.sql

**Schema of models:**
Schema is written in schema.yml. It defines:
- Model’s name
- Model’s description
- Column's name
- Column's description
- Data test


## TO USE DBT
1. Install required packages: 
dbt-core==1.7.13
dbt-postgres==1.7.13

2. Create profile.yml to store data warehouse connection settings.

3. Create dbt_project.yml to define:
- project name
- version
- profile name
- model paths and materialization settings

4. Define source and models

5. Use ‘dbt run’ to create datamart tables by SQL files in models folder.

6. Use ‘dbt test’ to test the model outputs. All tests are defined in the schema.yml file.

## Specific Benefits in the Project
1. Centralized Business Logic
All transformation logic is defined once in dbt models instead of being duplicated across BI tools. This ensures consistency across all dashboards.

2. Reusability
Core models can be reused across multiple charts without rewriting logic.

3. Maintainability
When business rules change, we update the dbt model once and all downstream reports automatically reflect the change.

4. Performance Optimization
We can pre-aggregate heavy transformations inside the data warehouse, reducing BI workload and improving dashboard performance.

5. Developer Experience:
Anyone comfortable with SQL can use dbt easily

## References
- [What is dbt?](https://docs.getdbt.com/docs/introduction)


